{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------\n",
    "# imports\n",
    "#---------------------------------------------------------------\n",
    "## conda activate bengali_ai\n",
    "import os, errno\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "import argparse\n",
    "from IPython.display import Video, Image\n",
    "import pytube\n",
    "from pytube import YouTube \n",
    "import zipfile\n",
    "from paddleocr import PaddleOCR\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from coreLib.utils import create_dir, is_supported, LOG_INFO, video_to_images, zipdir,\\\n",
    "    calculate_ssim, calculate_hausdorff_distance, inpaintredBox, viz_img_pair,\\\n",
    "         count_matched_bboxes, unique_frames_to_pdf\n",
    "from coreLib.config import Hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##---------------------------------\n",
    "data_dir        =   \"../datas/\"\n",
    "# isYouTube_video =   \"Compiler.mp4\" ## name of the presentation slide video\n",
    "isYouTube_video =   \"video_MLT_pdf.mp4\"\n",
    "# isYouTube_video =   \"https://www.youtube.com/watch?v=k6lCD0iVExo\"\n",
    "imgs_dir        =   \"../images/\"\n",
    "output_pdf_path =   \"../outputs/\"\n",
    "##---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whether \"imgs_dir\" is exist or not\n",
    "try:\n",
    "    os.makedirs(imgs_dir)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#LOG     :\u001b[0m\u001b[32m../datas/video_MLT_pdf.mp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " ## youtube video presentations\n",
    "if is_supported(isYouTube_video):\n",
    "    yt = YouTube(isYouTube_video)\n",
    "    # this method will download the highest resolution that video is available\n",
    "    yt_video = yt.streams.get_highest_resolution()\n",
    "    file_name = yt_video.download()\n",
    "\n",
    "    ## check whether file is exist or not\n",
    "    _file_name = file_name.split(\"/\")[-1]\n",
    "    file_exist = os.path.join(data_dir, _file_name)\n",
    "    if os.path.exists(file_exist):\n",
    "        os.remove(file_exist)\n",
    "    shutil.move(file_name, data_dir)\n",
    "    file_name = file_name.split(\"/\")[-1]\n",
    "    filename = os.path.join(data_dir, file_name)\n",
    "    LOG_INFO(f\"{filename}\",mcolor=\"green\") \n",
    "\n",
    "    #choosing dynamic fps based on video length for making computation fast without sacrificing vital informations\n",
    "    if(yt.length>=1800):\n",
    "        fps = 0.5\n",
    "    elif(yt.length<1800 and yt.length>1200):\n",
    "        fps = 0.75\n",
    "    else:\n",
    "        fps = 1.0\n",
    "else:\n",
    "    filename = os.path.join(data_dir, isYouTube_video) \n",
    "    LOG_INFO(f\"{filename}\",mcolor=\"green\") \n",
    "    video = cv2.VideoCapture(filename)\n",
    "    length = video.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    #choosing dynamic fps based on video length for making computation fast without sacrificing vital informations\n",
    "    if(length>=1800):\n",
    "        fps = 0.5\n",
    "    elif(length<1800 and length>1200):\n",
    "        fps = 0.75\n",
    "    else:\n",
    "        fps = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract key frames (setting fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#LOG     :\u001b[0m\u001b[32m1.0\u001b[0m\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mFrames Number: 232\u001b[0m\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mImages Path: ../images/video_MLT_pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Controllable Parameters\n",
    "hp = Hparams(filename, fps)\n",
    "LOG_INFO(f\"{hp.frames_per_second}\", mcolor=\"green\")\n",
    "\n",
    "if(hp.embed_video):\n",
    "    Video(filename,embed=True)\n",
    "\n",
    "## collecting least frames,required for finding unique slides\n",
    "frames, images_path = video_to_images(filename, imgs_dir, frames_per_second=hp.frames_per_second)\n",
    "LOG_INFO(f\"Frames Number: {len(frames)}\", mcolor=\"green\")\n",
    "LOG_INFO(f\"Images Path: {images_path}\", mcolor=\"green\")\n",
    "\n",
    "# Converting saved images directory into zip directory so that we can remove the folder later\n",
    "with zipfile.ZipFile('images.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir(images_path, zipf) # f'./images'\n",
    "\n",
    "## check images whether extracted: f'./images'\n",
    "images = os.listdir(images_path)\n",
    "images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning annotation from the extracted frames if there exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whether \"imgs_dir/rmv_annotation\" is exist or not\n",
    "temp_img_dir = os.path.join(imgs_dir, \"rmv_annotation\")\n",
    "try:\n",
    "    os.makedirs(temp_img_dir)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## annotation check & remove in the key frames\n",
    "for idx in range(len(images)):\n",
    "    img = cv2.imread(os.path.join(images_path, images[idx]))\n",
    "    # remove annotattion paintng\n",
    "    annot_res = inpaintredBox(img) \n",
    "    # cv2.imwrite(os.path.join(images_path, images[idx]), annot_res)\n",
    "    cv2.imwrite(os.path.join(temp_img_dir, images[idx]), annot_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER - 1 (apply SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtered images w.r.t. SSIM ### <<<----------------- FILTER - 1\n",
    "filtered_images_path = create_dir(imgs_dir, 'filtered')\n",
    "for idx in range(len(images) - 1):\n",
    "    img1 = cv2.imread(os.path.join(images_path, images[idx]))\n",
    "    img2 = cv2.imread(os.path.join(images_path, images[idx+1]))\n",
    "    score = calculate_ssim(hp, img1, img2, filtered_images_path, images[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtered images: f'./filtered' \n",
    "images = os.listdir(filtered_images_path)\n",
    "images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "with zipfile.ZipFile('filtered.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir(filtered_images_path, zipf) # f'./filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER - 2 (PaddleOCR: apply deep learning method for detecting unique frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/06/12 18:22:29] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='/home/rezwan/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='/home/rezwan/.paddleocr/whl/det/ml/Multilingual_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='ar', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='/home/rezwan/anaconda3/envs/bengali_ai/lib/python3.8/site-packages/paddleocr/ppocr/utils/dict/arabic_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='/home/rezwan/.paddleocr/whl/rec/arabic/arabic_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=True, use_dilation=False, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mConverting Images to pdf...\u001b[0m\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mPDF Created!\u001b[0m\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mPDF saved at: ../outputs/w.o.f_video_MLT_pdf.mp4.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## frame pair visualization\n",
    "\"\"\"\n",
    "    * Source: https://gist.github.com/mstankie/71e49f628beac320953e0460b8ee78c2\n",
    "    * Declare PaddleOCR class\n",
    "\"\"\"\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='ar',use_gpu = True)  \n",
    "if(hp.visualize_img_pairs):\n",
    "    viz_img_pair(ocr=ocr, images=images, filtered_images_path=filtered_images_path, i=4, j=5)\n",
    "\n",
    "## Detect all unique and informative images and save them in folder name unique\n",
    "unique_images_path = create_dir(imgs_dir, 'unique')  #f'./unique' \n",
    "'''\n",
    "    detect all unique and informative images and save them\n",
    "    images using db_resnet50 text detection algorithm of paddleocr\n",
    "'''\n",
    "files = os.listdir(filtered_images_path) # f'./filtered'\n",
    "files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "for i in range(len(files)-1):\n",
    "    prev = 1\n",
    "    for j in range(i+1, len(files)):\n",
    "        img=cv2.imread(os.path.join(filtered_images_path, files[i]))\n",
    "        img1=cv2.imread(os.path.join(filtered_images_path, files[j]))\n",
    "\n",
    "        try:\n",
    "            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "            img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            LOG_INFO(f\"{e}\",mcolor=\"red\") \n",
    "        \n",
    "        count,_ = count_matched_bboxes(hp, img,img1,detector=ocr)\n",
    "        if(count>= prev):\n",
    "            prev = count\n",
    "            cv2.imwrite(os.path.join(unique_images_path, files[i]), img1)\n",
    "        else:    \n",
    "            break\n",
    "\n",
    "## save unique Frames to pdf without final filtering # './unique'\n",
    "if(hp.withoutfinal_filter):\n",
    "    unique_frames_to_pdf(hp, unique_images_path, output_pdf_path, True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mFinal Filtering Length: 5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# final filtering\n",
    "\"\"\"\n",
    "*   at this point,it's possible that there still can exist few more redundant samples,\n",
    "    they don't always look like redundant because of complex animation or other stuffs but \n",
    "    according to their other key features like \"mid to near high bbox overlap coverage\", \n",
    "    they are redundant,we try to do one last filtering to detect and eliminate those redundant samples.\n",
    "\"\"\"\n",
    "images = os.listdir(unique_images_path)\n",
    "images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "for idx in range(len(images) - 1):\n",
    "    image = cv2.imread(os.path.join(unique_images_path, images[idx]))\n",
    "    image1 = cv2.imread(os.path.join(unique_images_path, images[idx+1]))\n",
    "    if(hp.is_ssim):\n",
    "        score = calculate_ssim(hp, image, image1, unique_images_path, images[idx], write_img=False)\n",
    "        if(score>hp.ssim_threshold):\n",
    "            os.remove(os.path.join(unique_images_path, images[idx]))\n",
    "            continue\n",
    "    try:\n",
    "        img=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        img1=cv2.cvtColor(image1,cv2.COLOR_BGR2RGB)\n",
    "    except Exception as e:\n",
    "        LOG_INFO(f\"{e}\",mcolor=\"red\") \n",
    "\n",
    "    count,minimum = count_matched_bboxes(hp, img,img1,detector=ocr)\n",
    "    if(count>= minimum * hp.conf_thr):\n",
    "        os.remove(os.path.join(unique_images_path, images[idx]))\n",
    "\n",
    "LOG_INFO(f\"Final Filtering Length: {len(os.listdir(unique_images_path))}\", mcolor=\"green\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final filter PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mConverting Images to pdf...\u001b[0m\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mPDF Created!\u001b[0m\n",
      "\u001b[32m#LOG     :\u001b[0m\u001b[32mPDF saved at: ../outputs/video_MLT_pdf.mp4.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Save final PDF\n",
    "unique_frames_to_pdf(hp, unique_images_path, output_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save as a unique.zip\n",
    "with zipfile.ZipFile('unique.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir(unique_images_path, zipf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra: removing frames and moving .zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## romove the the folders of extracted images\n",
    "if(hp.rmdir):\n",
    "    sub_folders_pathname = imgs_dir\n",
    "    sub_folders_list = glob.glob(sub_folders_pathname)\n",
    "    for sub_folder in sub_folders_list:\n",
    "        shutil.rmtree(sub_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datas/unique.zip'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## move .zip file to \"data_dir\" path\n",
    "# images.zip\n",
    "_file_exist = os.path.join(data_dir, 'images.zip')\n",
    "if os.path.exists(_file_exist):\n",
    "    os.remove(_file_exist)\n",
    "shutil.move(f'./images.zip', data_dir) \n",
    "\n",
    "# filtered.zip\n",
    "_file_exist = os.path.join(data_dir, 'filtered.zip')\n",
    "if os.path.exists(_file_exist):\n",
    "    os.remove(_file_exist)\n",
    "shutil.move(f'./filtered.zip', data_dir) \n",
    "\n",
    "# unique.zip\n",
    "_file_exist = os.path.join(data_dir, 'unique.zip')\n",
    "if os.path.exists(_file_exist):\n",
    "    os.remove(_file_exist)\n",
    "shutil.move(f'./unique.zip', data_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8bbe4fe62dd3d3fc281495773bee29eba95fa92a2e9ec6daef308576e4d9e58"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('bengali_ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
